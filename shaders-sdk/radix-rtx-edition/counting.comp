#version 460 core
#extension GL_GOOGLE_include_directive : enable

#define ENABLE_RTX_EDITION
#define EXTEND_LOCAL_GROUPS
#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"

#define COUNTING_STAGE
#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;
shared addrw_t localCounts[RADICES], addrL[VEC_SIZE][Wave_Size];
shared u32vec4 validAddressL[VEC_SIZE], prtMskL[VEC_SIZE][Wave_Size]; // but there is re-packed WARP with 4x values (and 16-bit packed keys)
shared m8pq utype_v keyL[VEC_SIZE][Wave_Size]; // here will 4x of uint8_t per thread
shared blocks_info blocks;

#define addrW addrL[w][ln]
#define keyM keys[push_block.Shift&1].data[addrW+i]

#ifndef INTERLEAVED_PARTITION
#define prtsumW prtsumL[w][ln]
#define fsLaneW fsLaneL[w][ln]
#define keyW keyL[w][ln]
#else
#define prtsumW prtsumL[w][ln][i]
#define fsLaneW fsLaneL[w][ln][i]
#define keyW keyL[w][ln][i]
#endif

#define bcount blocks.count

//const uint Wc = RADICES/Wave_Count;
//const uint BSIZE = min(Wc,Wave_Size);

// 
//layout ( binding = 0, set = InputKeys, rgba8ui ) uniform readonly workgroupcoherent uimageBuffer keys[];
  layout ( binding = 0, set = InputKeys, scalar ) readonly subgroupcoherent buffer KeysB { keytp_t data[]; } keys[];

// 
layout ( binding = 3, set = 0, scalar ) workgroupcoherent buffer HistogramB { uint counts[][RADICES]; };
//layout ( binding = 5, set = 0, scalar ) workgroupcoherent buffer ReferenceB { uint data[]; } offsets[];

// 
const lowp int blp = 10-bshift;
void main() {
    const lowp uint w = Wave_Idx, wT = w>>VEC_SHIF, wC = Wave_Count_RT>>VEC_SHIF, ln = Lane_Idx;//w * Wave_Size_RT + Lane_Idx;
    const lowp uint wID = Lane_Idx&7, smID = Lane_Idx>>3;

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) { const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) localCounts[radice] = 0u;
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(NumElements), bcount = min(blocks.count, 1048576u);
    subgroupBarrier();
    [[flatten]] IFANY (bcount <= 0) return;

    // permute blocks by partitions
    [[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint i=0;i<ivectr;i++) { addrW = blocks.offset + (Local_Idx<<bshift); }; };
    subgroupBarrier();

    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        btype_v predicate = btype_v(false);
        [[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint i=0;i<ivectr;i++) { 
            predicate wmI = lessThan(addrW+i, addrw_t(blocks.limit));
        }};
        IFALL(all(not(predicate))) break;

        // 
//#define ptnmask prtMskL[w][uint(keyW)]
#define prmskM prtMskL[w][Lane_Idx]
#define validM validAddressL[w]

#define prmskL prmskM[li]
#define validL validM[li]
const lowp uint bms = 31;
//const lowp uint bms = 63;

        // 
        [[flatten]] if (w < VEC_SIZE) {
            [[unroll]] for (lowp uint i=0;i<ivectr;i++) { keyW = extractKey(keyM, push_block.Shift); };
            addrW += (Wave_Size_RT * VEC_SIZE)<<bshift, validM = fwp_rtx_u4(predicate);
        };

        // 
        //[[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint i=0;i<ivectr;i++) { const lowp uint li = i*Wave_Size_RT + Lane_Idx; ptnmask = DEF_MASK; }; };
        //[[flatten]] if (w < VEC_SIZE) { ptnmask = DEF_MASK; };
        [[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint li=0;li<ivectr;li++) { const lowp uint ln = (li<<3u)|(Lane_Idx>>2u), i = wID&3u;
            { prmskL = sgpexc(keyW) & validL; }; // more shorter way
        //}};
        //[[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint i=0;i<ivectr;i++) { const lowp uint li = bitfieldInsert(i,ln,bshift,blp), bnt = bitcnt(ptnmask);
            [[flatten]] if (Lane_Idx == lsb(prmskL) && prmskL > 0u) { atomicAdd(localCounts[uint(keyW)], bitcnt(prmskL), gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsRelaxed); };
        }};
        //[[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint i=0;i<ivectr;i++) { addrW += ( (Wave_Size_RT<<bshift) << VEC_SHIF ); }};
    };
    subgroupBarrier();
    
    // resolve histograms 
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) {
        const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) { counts[gl_WorkGroupID.x][radice] = localCounts[radice+0u]; };
    };
};
