#version 460 core
#extension GL_GOOGLE_include_directive : enable

#define EXTEND_LOCAL_GROUPS
#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"

#define HISTOGRAM_STAGE
#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;
shared uint localCounts[RADICES];

// planned 64-wide for Turing
shared m8pq utype_t keyL[VEC_SIZE][Wave_Size];

shared uint addrL[VEC_SIZE*Wave_Size], prtscnL[VEC_SIZE][Wave_Size];
shared lowp uint prtsumL[VEC_SIZE][Wave_Size], fsLaneL[VEC_SIZE][Wave_Size];
shared highp uvec4[VEC_SIZE] validAddressL;
shared blocks_info blocks;

//#ifndef ENABLE_SUBGROUP_PARTITION_SORT
//shared highp uvec4 bltL[VEC_SIZE][Wave_Size];
//#define blt bltL[w][Lane_Idx]
//#endif

#define addrW addrL[Local_Idx]
#define keyW keyL[w][Lane_Idx]
#define prtscnW prtscnL[w][Lane_Idx]
#define prtsumW prtsumL[w][Lane_Idx]
#define fsLaneW fsLaneL[w][Lane_Idx]

#define validAddress subgroupInverseBallot(validAddressL[w])
#define bcount blocks.count

const uint Wc = RADICES/Wave_Count;
const uint BSIZE = min(Wc,Wave_Size);

// 
//layout ( binding = 0, set = InputKeys, rgba8ui ) uniform readonly workgroupcoherent uimageBuffer keys[];
  layout ( binding = 0, set = InputKeys, scalar ) readonly workgroupcoherent buffer KeysB { uint data[]; } keys[];

// pointer buffers
layout ( binding = 3, set = 0, scalar ) workgroupcoherent buffer HistogramB { uint counts[][RADICES]; };
layout ( binding = 5, set = 0, scalar ) workgroupcoherent buffer ReferenceB { uint data[]; } offsets[];

// 
void main() {
    //const m8pq utype_t Radice_Idx = utype_t(gl_WorkGroupID.y * Wave_Count_RX + Wave_Idx);
    const lowp uint w = Wave_Idx, Wr = Wc * w;

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) {
        const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) localCounts[radice+0u] = 0u;
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(NumElements), bcount = min(blocks.count, 1048576u);
    LGROUP_BARRIER
    [[flatten]] IFANY (bcount <= 0) return;

    // calculate blocks
    [[flatten]] if (w < VEC_SIZE) { addrW = Local_Idx + blocks.offset; };

    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        [[flatten]] if (w < VEC_SIZE) validAddressL[w] = subgroupBallot(lessThan(addrW, blocks.limit));
        //IFALL(all(not(validAddress))) break;
        
        // encode radice for read-only
        [[flatten]] if (w < VEC_SIZE) { // 
            keyW = utype_t(extractKey(keys[push_block.Shift&1].data[addrW], push_block.Shift));
        };

#ifdef ENABLE_SUBGROUP_PARTITION_SORT // subgroup partition supported now 
        [[flatten]] if (w < VEC_SIZE) {
            const highp uvec4 prtmask = subgroupPartitionNV(keyW)&validAddressL[w];
            prtscnW = utype_t(subgroupBallotExclusiveBitCount(prtmask)), prtsumW = utype_t(subgroupBallotBitCount(prtmask)), fsLaneW = utype_t(subgroupBallotFindLSB(prtmask)); 
        };
#else
        LGROUP_BARRIER
        [[unroll]] for (lowp uint w=0;w<VEC_SIZE;w++) {
    #ifdef SIMPLER_SORT
            [[flatten]] if (keyW == Wr && validAddress) {
                const highp uvec4 prtmask = subgroupBallot(true);
                prtscnW = utype_t(subgroupBallotExclusiveBitCount(prtmask)), prtsumW = utype_t(subgroupBallotBitCount(prtmask)), fsLaneW = utype_t(subgroupBallotFindLSB(prtmask));
            };
    #else
            bool found = !validAddress || keyW<Wr || keyW>=(Wr+Wc);
            for (lowp uint t=0;t<BSIZE;t+=1u) {
                [[flatten]] if (!found && (found = subgroupMin(keyW)==keyW)) {
                    const highp uvec4 prtmask = subgroupBallot(true);
                    prtscnW = utype_t(subgroupBallotExclusiveBitCount(prtmask)), prtsumW = utype_t(subgroupBallotBitCount(prtmask)), fsLaneW = utype_t(subgroupBallotFindLSB(prtmask));
                };
                [[flatten]] if ( subgroupAll(found) ) { break; };
            };
    #endif
        };
#endif

        // critical block
        LGROUP_BARRIER
        [[flatten]] if (w == 0u) {
            [[unroll]] for (lowp uint w=0;w<VEC_SIZE;w++) { // critically calculate partition offset
                uint cntl = 0u; [[flatten]] if (fsLaneW == Lane_Idx && validAddress) { cntl = add(localCounts[uint(keyW)], prtsumW); };
                prtscnW += subgroupShuffle(cntl,fsLaneW);
            }};

        // 
        LGROUP_BARRIER
        [[flatten]] if (w < VEC_SIZE) {
            [[flatten]] if (validAddress) { offsets[0].data[addrW] = prtscnW; }; // local offset scan without global partition (needs to calculate and permute)
            addrW += ( Wave_Size_RT << VEC_SHIF );
        };
    };

    LGROUP_BARRIER
    
    // resolve histograms 
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) {
        const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) { counts[gl_WorkGroupID.x][radice] = localCounts[radice+0u]; };
    };
};
