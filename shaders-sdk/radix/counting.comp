#version 460 core
#extension GL_GOOGLE_include_directive : enable

#define EXTEND_LOCAL_GROUPS
#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"

#define COUNTING_STAGE
#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;
shared uint localCounts[RADICES];

// 
shared m8pq utype_v keyL[VEC_SIZE][Wave_Size];
shared addrw_v prtsumL[VEC_SIZE][Wave_Size], fsLaneL[VEC_SIZE][Wave_Size], addrL[VEC_SIZE*Wave_Size];
shared bqualf uvec4[VEC_SIZE] validAddressL;

shared blocks_info blocks;

#define addrW addrL[Local_Idx]
#define keyM keys[push_block.Shift&1].data[addrW.x>>bshift]//kymL[Local_Idx]

#ifndef INTERLEAVED_PARTITION
#define prtsumW prtsumL[wcmsk][Lane_Idx]
#define fsLaneW fsLaneL[wcmsk][Lane_Idx]
#define keyW keyL[wcmsk][Lane_Idx]
#else
#define prtsumW prtsumL[wcmsk][Lane_Idx][i]
#define fsLaneW fsLaneL[wcmsk][Lane_Idx][i]
#define keyW keyL[wcmsk][Lane_Idx][i]
#endif

#define bcount blocks.count

const uint Wc = RADICES/Wave_Count;
const uint BSIZE = min(Wc,Wave_Size);

// 
//layout ( binding = 0, set = InputKeys, rgba8ui ) uniform readonly workgroupcoherent uimageBuffer keys[];
  layout ( binding = 0, set = InputKeys, scalar ) readonly subgroupcoherent buffer KeysB { keytp_v data[]; } keys[];

// 
layout ( binding = 3, set = 0, scalar ) workgroupcoherent buffer HistogramB { uint counts[][RADICES]; };
//layout ( binding = 5, set = 0, scalar ) workgroupcoherent buffer ReferenceB { uint data[]; } offsets[];

// 
void main() {
    const lowp uint w = Wave_Idx, wT = w>>VEC_SHIF, wC = Wave_Count_RT>>VEC_SHIF;

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) { const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) localCounts[radice] = 0u;
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(NumElements), bcount = min(blocks.count, 1048576u);
    subgroupBarrier();
    [[flatten]] IFANY (bcount <= 0) return;

    // permute blocks by partitions
    [[flatten]] if (w < VEC_SIZE) { addrW = ((Local_Idx)<<bshift) + addrw_seq + blocks.offset; };
    subgroupBarrier();

    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        btype_v predicate = lessThan(addrW, addrw_v(blocks.limit));
        [[flatten]] if (w < VEC_SIZE) { prtsumL[w][Lane_Idx] = addrw_v(0u); };
        IFALL(all(not(predicate))) break;

#ifndef INTERLEAVED_PARTITION
        [[flatten]] if (w < VEC_SIZE && predicate) { keyW = extractKey(keyM, push_block.Shift), validAddressL[w] = sgrblt(true); };
    #ifdef ENABLE_SUBGROUP_PARTITION_SORT
        bqualf uvec4 prtmask = uvec4(0u);
    #else
        #define prtmask prtMskW[uint(keyW)]
    #endif
#else
        [[flatten]] if (w < VEC_SIZE) { keyL[w][Lane_Idx] = extractKey(keyM, push_block.Shift), validAddressL[w] = sgrblt(predicate); };
        highp uvec4 prtmskM[ivectr];
        #define prtmask prtmskM[i]
#endif

#ifdef ENABLE_SUBGROUP_PARTITION_SORT // subgroup partition supported now (interleaving currently not supported, need HW support)
    #ifndef INTERLEAVED_PARTITION
        [[flatten]] if (w < VEC_SIZE) {
            prtmask = sgrprt(keyW) & validAddressL[w], prtsumW = subgroupBallotBitCount(prtmask), fsLaneW = subgroupBallotFindLSB(prtmask);
        };
    #else
        [[flatten]] if (w < VEC_SIZE) { [[unroll]] for (lowp uint i=0;i<ivectr;i++) {
            prtmask = (interleave32x2(sgrprt(keyW))<<i) & validAddressL[w], prtsumW = bitcnt(prtmask), fsLaneW = lsb(prtmask)>>bshift;
        }};
    #endif
#else
        [[unroll]] for (lowp uint r=0;r<RADICES;r+=wC) { [[flatten]] if (keyW == (r+wT) && subgroupInverseBallot(validAddressL[wcmsk])) {
            const bqualf uvec4 prtmskM = sgrblt(true);
            prtsumW = subgroupBallotBitCount(prtmskM), fsLaneW = readFLane(Lane_Idx);
        }};
#endif

        [[flatten]] if (w < VEC_SIZE) {
#ifndef INTERLEAVED_PARTITION
            [[flatten]] if (fsLaneW == Lane_Idx && subgroupInverseBallot(validAddressL[wcmsk])) { atomicAdd(localCounts[uint(keyW)], prtsumW, gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsRelaxed); };
#else
            [[unroll]] for (lowp uint i=0;i<ivectr;i++) {
                [[flatten]] if (fsLaneW == Lane_Idx && predicate[i] && prtsumW > 0) { atomicAdd(localCounts[uint(keyW)], prtsumW, gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsRelaxed); };};
#endif
            addrW += ( (Wave_Size_RT<<bshift) << VEC_SHIF );
        };
    };
    subgroupBarrier();
    
    // resolve histograms 
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) {
        const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) { counts[gl_WorkGroupID.x][radice] = localCounts[radice+0u]; };
    };
};
