#version 460 core
#extension GL_GOOGLE_include_directive : enable

#define EXTEND_LOCAL_GROUPS
#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"

#include "./includes.glsl"


layout (local_size_x = BLOCK_SIZE) in;
shared uint localCounts[RADICES], localPartitions[RADICES];

// 
shared m8pq utype_v keyL[VEC_SIZE][Wave_Size];
shared keytp_v kymL[VEC_SIZE*Wave_Size];

shared addrw_v addrL[VEC_SIZE*Wave_Size], prtsumL[VEC_SIZE][Wave_Size];
shared lowp addrw_v prtscnL[VEC_SIZE][Wave_Size], fsLaneL[VEC_SIZE][Wave_Size];

shared vqualf uvec4 prtMskL[VEC_SIZE][RADICES];
shared vqualf uvec4[VEC_SIZE] validAddressL;

shared blocks_info blocks;

#define addrW addrL[Local_Idx]
#define prtMskW prtMskL[w]
#define prtscnW prtscnL[w][Lane_Idx]
#define prtsumW prtsumL[w][Lane_Idx]
#define fsLaneW fsLaneL[w][Lane_Idx]
#define keyW keyL[w][Lane_Idx]
#define keyM keys[push_block.Shift&1].data[addrW.x>>bshift]//kymL[Local_Idx]
//#define validAddress subgroupInverseBallot(validAddressM)
//#define validAddress subgroupInverseBallot(validAddressL[w])
#define bcount blocks.count

const uint Wc = RADICES/Wave_Count;
const uint BSIZE = min(Wc,Wave_Size);

// 
//layout ( binding = 0, set = InputKeys, rgba8ui ) uniform workgroupcoherent uimageBuffer keys[];
  layout ( binding = 0, set = InputKeys, scalar ) readonly subgroupcoherent buffer KeysB { keytp_v data[]; } keys[];
  layout ( binding = 0, set = InputKeys, scalar )          subgroupcoherent buffer KeysOutB { keytp_t data[]; } keysOut[];

// 
layout ( binding = 4, set = 0, scalar ) readonly workgroupcoherent buffer PrefixSumB { uint partitions[][RADICES]; };
//layout ( binding = 5, set = 0, scalar ) readonly workgroupcoherent buffer ReferenceB { uint data[]; } offsets[];

// 
void main() {
    //const m8pq utype_t Radice_Idx = utype_t(gl_WorkGroupID.y * Wave_Count_RX + w);
    //const lowp uint w = Wave_Idx, Wr = Wc * w;
    const lowp uint w = Wave_Idx, Wr = Wc * w, Wx = tiled(Wave_Size_RT, Wave_Count_RT);

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) { const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) { localPartitions[radice] = partitions[gl_WorkGroupID.x][radice], localCounts[radice] = 0u; };
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(NumElements), bcount = min(blocks.count, 1048576u);
    subgroupBarrier();
    [[flatten]] IFANY (bcount <= 0) return;

    // permute blocks by partitions
    [[flatten]] if (w < VEC_SIZE) { 
        addrW = ((Local_Idx)<<bshift) + addrw_seq + blocks.offset;
    };
    subgroupBarrier();
    
    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        btype_v predicate = lessThan(addrW, addrw_v(blocks.limit));
        [[flatten]] if (w < VEC_SIZE) { prtsumW = addrw_v(0u), prtscnW = addrw_v(0u); };
        IFALL(all(not(predicate))) break;
        [[flatten]] if (w < VEC_SIZE) { keyW = extractKey(keyM, push_block.Shift), validAddressL[w] = sgr_blt(predicate); };

#ifdef ENABLE_SUBGROUP_PARTITION_SORT // subgroup partition supported now
    #ifndef INTERLEAVED_PARTITION
        [[flatten]] if (w < VEC_SIZE) {
            const bqualf uvec4 prtmask = sgr_prt(keyW) & validAddressL[w]; // 
            [[flatten]] if ((fsLaneW = addrw_v(subgroupBallotFindLSB(prtmask))) == Lane_Idx) { prtMskW[uint(keyW)] = prtmask; };
        };
    #else // subgroup interleaving support (NVIDIA)
        #define prtmask prtmaskM[i]
        uvec4 prtmaskM[ivectr];
        [[unroll]] for (lowp uint i=0;i<ivectr;i++) {
            prtmask = (encodeMorton32x2(sgr_prt(keyW[i]))<<i) & validAddressL[w];
            [[flatten]] if (w < VEC_SIZE) { fsLaneW[i] = addrw_t(lsb(prtmask)), prtsumW[i] = addrw_t(bitcnt(prtmask))>>bshift; };
            [[flatten]] if (fsLaneW[i] == Lane_Idx) { prtMskW[uint(keyW[i])] = uvec4(0u); };
        };
        [[unroll]] for (lowp uint i=0;i<ivectr;i++) {
            [[flatten]] if (fsLaneW[i] == Lane_Idx) { prtMskW[uint(keyW[i])] |= prtmask; };
        };
    #endif
#else
        // 
        [[flatten]] if (w < VEC_SIZE) {[[unroll]] for (lowp uint r=0;r<RADICES;r+=1u) { [[flatten]] if (keyW == r && predicate) {
            const bqualf uvec4 prtmask = sgr_blt(true);
            [[flatten]] if ((fsLaneW = readFLane(Lane_Idx)) == Lane_Idx) { prtMskW[uint(keyW)] = prtmask; };
        };};};
#endif

#ifndef INTERLEAVED_PARTITION
        // 
        [[flatten]] if (w < VEC_SIZE) { prtscnW = addrw_v(subgroupBallotExclusiveBitCount(prtMskW[uint(keyW)])), prtsumW = addrw_v(subgroupBallotBitCount(prtMskW[uint(keyW)])); };

        // critical block 
        subgroupBarrier();
        [[flatten]] if (w == 0u) { [[unroll]] for (lowp uint w=0;w<VEC_SIZE;w++) { // critically calculate partition offset
            uint count = localCounts[uint(keyW)];
            [[flatten]] if (fsLaneW == Lane_Idx) { atomicAdd(localCounts[uint(keyW)], prtsumW, gl_ScopeSubgroup, gl_StorageSemanticsShared, gl_SemanticsRelaxed); };
            prtsumW = count;
        }};
#else   // subgroup interleaving support (NVIDIA)
        // calculate bitcount and bitscan (for bitscan required re-form above)
        [[unroll]] for (lowp uint i=0;i<ivectr;i++) {
            [[flatten]] if (w < VEC_SIZE) { prtscnW[i] = addrw_t(bitcnt(prtMskW[uint(keyW[i])]&genLt2Mask(i))); };
        };

        subgroupBarrier();
        [[flatten]] if (w == 0u) { [[unroll]] for (lowp uint w=0;w<VEC_SIZE;w++) { // critically calculate partition offset
            addrw_v count = addrw_v(0u); [[unroll]] for (lowp uint i=0;i<ivectr;i++) count[i] = localCounts[uint(keyW[i])];
            [[unroll]] for (lowp uint i=0;i<ivectr;i++) {
                [[flatten]] if (fsLaneW[i] == Lane_Idx) { atomicAdd(localCounts[uint(keyW[i])], prtsumW[i], gl_ScopeSubgroup, gl_StorageSemanticsShared, gl_SemanticsRelaxed); };
                prtsumW[i] = count[i];
            };
        }};
#endif

        // 
        subgroupBarrier();
        [[flatten]] if (w < VEC_SIZE) { // TODO: local sorting to avoid cache missing or using TBO
#ifdef INTERLEAVED_PARTITION
            [[unroll]] for (lowp uint i=0;i<ivectr;i++) { [[flatten]] if (predicate[i]) { // copy from backup to inner store
                keysOut[1-(push_block.Shift&1)].data[prtscnW[i] + prtsumW[i] + localPartitions[uint(keyW[i])]] = keyM[i]; 
            };};
#else
            [[flatten]] if (subgroupInverseBallot(validAddressL[w])) { keysOut[1-(push_block.Shift&1)].data[prtscnW + prtsumW + localPartitions[uint(keyW)]] = keyM; }; // copy from backup to inner store
#endif
            subgroupBarrier(); // barrier faster memory operation
            addrW += ( (Wave_Size_RT<<bshift) << VEC_SHIF );
        };
    };
};
