#version 460 core
#extension GL_GOOGLE_include_directive : enable

#define EXTEND_LOCAL_GROUPS
#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"

#include "./includes.glsl"

layout (local_size_x = VEC_SIZE*Wave_Size) in;
shared uint localPartitions[RADICES];

// planned 64-wide for Turing
shared m8pq utype_t keyL[VEC_SIZE][Wave_Size];

#ifdef SIMPLER_READ_U8
shared u8vec4 keymL[VEC_SIZE*Wave_Size];
#else
shared uint keymL[VEC_SIZE*Wave_Size];
#endif

shared uint addrL[VEC_SIZE*Wave_Size], prtscnL[VEC_SIZE][Wave_Size];//, ptrL[VEC_SIZE][Wave_Size];
shared highp uvec4[VEC_SIZE] validAddressL;
shared blocks_info blocks;

#define prtscnW prtscnL[w][Lane_Idx]
#define addrW addrL[Local_Idx]
#define keyW keyL[w][Lane_Idx]
#define ptrW ptrL[w][Lane_Idx]
#define keymW keymL[Local_Idx]
#define validAddress subgroupInverseBallot(validAddressL[w])
#define bcount blocks.count

const uint Wc = RADICES/Wave_Count;
const uint BSIZE = min(Wc,Wave_Size);

// 
#ifdef SIMPLER_READ_U8
layout ( binding = 0, set = InputKeys, scalar ) devicecoherent buffer KeyInB { u8vec4 data[]; } keys[];
#else
layout ( binding = 0, set = InputKeys, scalar ) devicecoherent buffer KeyInB { KEYTYPE data[]; } keys[];
#endif

// 
layout ( binding = 4, set = 0, scalar ) readonly workgroupcoherent buffer PrefixSumB { uint partitions[][RADICES]; };
layout ( binding = 5, set = 0, scalar ) readonly workgroupcoherent buffer ReferenceB { uint data[]; } offsets[];

// 
void main() {
    //const m8pq utype_t Radice_Idx = utype_t(gl_WorkGroupID.y * Wave_Count_RX + w);
    const lowp uint w = Wave_Idx, Wr = Wc * w;

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=gl_WorkGroupSize.x) { const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) { localPartitions[radice+0u] = partitions[gl_WorkGroupID.x][radice]; };
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(NumElements), bcount = min(blocks.count, 1048576u);
    LGROUP_BARRIER
    [[flatten]] IFANY (bcount <= 0) return;

    // permute blocks by partitions
    [[flatten]] if (w < VEC_SIZE) { addrW = Local_Idx + blocks.offset; };
    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        [[flatten]] if (w < VEC_SIZE) validAddressL[w] = subgroupBallot(lessThan(addrW, blocks.limit));
        IFALL(all(not(validAddress))) break;

        [[flatten]] if (w < VEC_SIZE) {
            keymW = keys[push_block.Shift&1].data[addrW];
#ifdef SIMPLER_READ_U8
            keyW = validAddress ? utype_t(keymW[push_block.Shift]) : utype_t(RADICES_MASK),
#else
            keyW = validAddress ? utype_t(BFE(keymW,int(push_block.Shift)*BITS_PER_PASS,BITS_PER_PASS)) : utype_t(RADICES_MASK),
#endif
            prtscnW = offsets[0].data[addrW];
        };

        // TODO: local sorting to avoid cache missing or using TBO 
        [[flatten]] if (w < VEC_SIZE) {
            [[flatten]] if (validAddress) { prtscnW += localPartitions[uint(keyW)], keys[1-(push_block.Shift&1)].data[prtscnW] = keymW; }; // copy from backup to inner store
            addrW += ( Wave_Size_RT << VEC_SHIF );
        };
    };
};
