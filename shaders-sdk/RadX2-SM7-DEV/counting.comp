#version 460 core
#extension GL_GOOGLE_include_directive : enable

#define EXTEND_LOCAL_GROUPS
#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"

#define COUNTING_STAGE
#include "./includes.glsl"


layout (local_size_x = Wave_Size, local_size_y = ivectr, local_size_z = Wave_Count) in;
shared addrw_t localCounts[RADICES], addrL[VEC_SIZE][Wave_Size*ivectr];
shared sgp_tp validAddressL[VEC_SIZE], prtMskL[RADICES][VEC_SIZE];
shared utype_t keyL[VEC_SIZE][Wave_Size*ivectr];
shared blocks_info blocks;

#define addrW addrL[w][lf+ln]
#define keyM keys[push_block.Shift&1].data[addrW]
#define prtsumW prtsumL[w][lf+ln]
#define fsLaneW fsLaneL[w][lf+ln]
#define keyW keyL[w][lf+ln]
#define bcount blocks.count

// 
//layout ( binding = 0, set = InputKeys, rgba8ui ) uniform readonly workgroupcoherent uimageBuffer keys[];
  layout ( binding = 0, set = InputKeys, scalar ) readonly subgroupcoherent buffer KeysB { keytp_t data[]; } keys[];
  layout ( binding = 3, set = 0, scalar ) workgroupcoherent buffer HistogramB { uint counts[][RADICES]; };
//layout ( binding = 5, set = 0, scalar ) workgroupcoherent buffer ReferenceB { uint data[]; } offsets[];

// 
const lowp int blp = 10-bshift;
void main() {
    const lowp uint w = gl_LocalInvocationID.z, ln = gl_LocalInvocationID.x, cw = gl_LocalInvocationID.y, lf = cw*Wave_Size, vf = cw*VEC_SIZE;
    //const lowp uint lv = Local_Idx&3u, lc = Local_Idx>>2u; // get vec4/sequence count representations 

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=BLOCK_SIZE_RT) { const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) localCounts[radice] = 0u;
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(NumElements), bcount = min(blocks.count, 1048576u);
    subgroupBarrier();
    [[flatten]] IFANY (bcount <= 0) return;

    // permute blocks by partitions
    [[flatten]] if (w < VEC_SIZE) { addrW = blocks.offset + (w * ivectr * Wave_Size + lf+ln); };
    subgroupBarrier();

    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        const bool predicate = lessThan(addrW, addrw_t(blocks.limit));
        IFALL(all(not(predicate))) break;

        // 
#define prmskM prtMskL[uint(keyW)][w]
#define validM validAddressL[w]
#define prmskL prmskM[cw]
#define validL validM[cw]
#define prmskW pack32(prmskM)

        // 
        [[flatten]] if (w < VEC_SIZE) {
            validL = sgpble(predicate), keyW = extractKey(keyM, push_block.Shift), prmskM = u16vec2(0u), prmskL = sgpexc(keyW) & validL, addrW += BLOCK_SIZE_RT;
            [[flatten]] if ((ln+lf) == lsb(prmskW) && prmskW > 0u) { atomicAdd(localCounts[uint(keyW)], bitcnt(prmskW), gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsRelaxed); };
        };
    };
    subgroupBarrier();

    // resolve histograms 
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=BLOCK_SIZE_RT) {
        const lowp uint radice = rk + Local_Idx;
        [[flatten]] if (radice < RADICES) { counts[gl_WorkGroupID.x][radice] = localCounts[radice+0u]; };
    };
};
